# Resilient Multi-Master State Synchronization over Unreliable Mesh Networks

A production-quality distributed system implementation for achieving eventual consistency across mesh networks using UDP-based communication, CRDT conflict resolution, and custom reliability protocols.

## ğŸ¯ Project Overview

This system enables multiple nodes in an unreliable network to maintain synchronized state without centralized coordination. Each node acts as both client and server, collaborating to achieve eventual consistency despite:

- **Packet Loss**: Up to 30%+ packet loss rates
- **High Latency & Jitter**: Variable network delays
- **Node Crashes**: Graceful handling of node failures
- **Network Partitions**: Recovery after partition healing

## ğŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MeshSyncNode                       â”‚
â”‚  (Main Orchestrator)                            â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  CRDTState   â”‚  â”‚ PeerManager â”‚            â”‚
â”‚  â”‚              â”‚  â”‚             â”‚            â”‚
â”‚  â”‚ â€¢ LWW-Registerâ”‚  â”‚ â€¢ Discoveryâ”‚            â”‚
â”‚  â”‚ â€¢ Vector Clockâ”‚  â”‚ â€¢ Heartbeatsâ”‚           â”‚
â”‚  â”‚ â€¢ Tombstones  â”‚  â”‚ â€¢ Failure Detâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   ReliableUDPSocket          â”‚              â”‚
â”‚  â”‚                              â”‚              â”‚
â”‚  â”‚ â€¢ Packet Sequencing          â”‚              â”‚
â”‚  â”‚ â€¢ ACK/Retransmission         â”‚              â”‚
â”‚  â”‚ â€¢ Checksum Verification      â”‚              â”‚
â”‚  â”‚ â€¢ Exponential Backoff        â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. **ReliableUDPSocket** (`reliable_udp.py`)

Provides application-layer reliability over UDP:

- **Packet Structure**: 
  ```
  [Version(1) | Type(1) | SeqNum(4) | Checksum(4)] + JSON Payload
  ```
- **Sequencing**: Monotonically increasing sequence numbers
- **Acknowledgements**: Explicit ACK packets for reliable delivery
- **Retransmissions**: Exponential backoff (0.5s â†’ 8s max)
- **Duplicate Detection**: Tracks received sequences per peer
- **Corruption Detection**: MD5 checksum validation

**Packet Types**:
- `DATA`: General data packets
- `ACK`: Acknowledgement packets
- `SYNC_REQUEST`: Request peer's full state
- `SYNC_RESPONSE`: Send full state to peer
- `HEARTBEAT`: Keepalive messages
- `DISCOVERY`: Peer discovery and gossip

### 2. **CRDTState** (`crdt_state.py`)

Implements Conflict-free Replicated Data Types:

- **LWW-Register**: Last-Write-Wins for key-value pairs
- **Vector Clocks**: Tracks causality between nodes
- **Tombstones**: Soft deletes with timestamp tracking
- **Deterministic Conflict Resolution**: Uses (timestamp, node_id) for tie-breaking

**Merge Algorithm**:
```python
For each key in remote_state:
    if remote_timestamp > local_timestamp:
        accept remote value
    elif remote_timestamp == local_timestamp:
        use node_id as tiebreaker (higher wins)
    check tombstones with same logic
```

### 3. **PeerManager** (`peer_manager.py`)

Handles peer lifecycle management:

- **Discovery**: Bootstrap-based peer discovery with gossip
- **Health Monitoring**: Periodic heartbeats (default: 5s)
- **Failure Detection**: Timeout-based (default: 15s)
- **Sync Scheduling**: Tracks last sync time per peer

### 4. **MeshSyncNode** (`mesh_node.py`)

Main orchestration layer:

- **Background Threads**:
  - Sync Loop: Periodically synchronizes with peers
  - Heartbeat Loop: Sends keepalives
  - Discovery Loop: Broadcasts to bootstrap peers
  
- **State Operations**: `set()`, `get()`, `delete()`
- **Callbacks**: `on_state_change` for monitoring

## ğŸ“¦ Installation

### Prerequisites

- Python 3.7+
- No external dependencies required (uses only stdlib)

### Setup

```bash
git clone <repository>
cd mesh-sync
chmod +x *.py
```

## ğŸš€ Usage

### Basic Example: Two-Node Cluster

**Terminal 1 - Start Node 1:**
```bash
python3 example_basic.py node1 5001
```

**Terminal 2 - Start Node 2 (connects to Node 1):**
```bash
python3 example_basic.py node2 5002 localhost 5001
```

**Interactive Commands:**
```
[node1]> set mykey myvalue
[node1]> list
[node1]> status
[node1]> get mykey
[node1]> delete mykey
```

### Multi-Node Simulation

Automatically starts 5 nodes with random operations:

```bash
python3 test_simulation.py
```

This will:
- Start 5 nodes in a mesh topology
- Automatically generate random set/delete operations
- Display cluster state every 10 seconds
- Show convergence status

### Reliability Testing

Test packet loss and partition recovery:

```bash
python3 test_reliability.py
```

Tests include:
1. **Convergence Under 30% Packet Loss**
2. **Network Partition Recovery**

## ğŸ”¬ Key Features

### 1. Eventual Consistency

All nodes eventually converge to the same state through:
- Periodic synchronization
- Anti-entropy sync requests
- Gossip-based peer discovery

### 2. Partition Tolerance

System handles network partitions gracefully:
- Nodes operate independently during partition
- Automatic reconciliation after healing
- CRDT guarantees conflict-free merge

### 3. Reliability Over UDP

Custom reliability protocol provides:
- **Automatic Retransmissions**: Up to 5 retries with exponential backoff
- **In-Order Delivery**: Not required, CRDT handles out-of-order
- **Loss Detection**: ACK timeouts trigger retries
- **Corruption Detection**: Checksum validation

### 4. Conflict Resolution

LWW-Register with deterministic tie-breaking:
```python
# Conflict: Two nodes write to same key simultaneously
node1: set('key', 'A') at t=1000.5, node_id='node1'
node2: set('key', 'B') at t=1000.5, node_id='node2'

# Resolution: node_id 'node2' > 'node1' (lexicographic)
# Winner: 'B' from node2
```

### 5. Concurrency Control

Thread-safe operations:
- RLocks for state access
- Atomic sequence number generation
- Safe concurrent packet handling

## ğŸ“Š Performance Characteristics

### Network Overhead

- **Heartbeat**: ~100 bytes every 5 seconds per peer
- **Sync Request**: ~200 bytes
- **Sync Response**: Depends on state size (typically < 64KB)
- **ACKs**: ~50 bytes per reliable packet

### Convergence Time

With default settings:
- **2 nodes**: ~2-5 seconds
- **5 nodes**: ~5-15 seconds
- **With 30% packet loss**: ~10-30 seconds

Factors:
- Sync interval (default: 10s)
- Heartbeat interval (default: 5s)
- Network conditions

### Scalability

Tested configurations:
- **Nodes**: Up to 10 nodes
- **State Size**: Up to 1000 keys
- **Packet Loss**: Up to 50%

## ğŸ§ª Testing

### Unit Testing Pattern

```python
# Test convergence
node1.set('key', 'value1')
time.sleep(sync_interval * 2)
assert node2.get('key') == 'value1'

# Test conflict resolution
node1.set('shared', 'A')
node2.set('shared', 'B')
time.sleep(sync_interval * 2)
# Both should have same value (LWW winner)
assert node1.get('shared') == node2.get('shared')
```

### Network Simulation

Use `UnreliableNetworkSimulator` to test under adverse conditions:

```python
from test_reliability import UnreliableNetworkSimulator

# Simulate 30% packet loss
sim = UnreliableNetworkSimulator(
    node.udp.socket, 
    packet_loss_rate=0.3
)
```

## ğŸ”§ Configuration

### Node Parameters

```python
node = MeshSyncNode(
    node_id='unique_id',
    port=5000,
    sync_interval=10.0,      # How often to sync (seconds)
    heartbeat_interval=5.0   # How often to heartbeat
)
```

### Reliability Settings

In `reliable_udp.py`:
```python
MAX_RETRIES = 5              # Max retransmission attempts
INITIAL_TIMEOUT = 0.5        # Initial retry timeout (seconds)
MAX_TIMEOUT = 8.0            # Maximum retry timeout
```

### Peer Failure Detection

In `peer_manager.py`:
```python
peer_timeout = 15.0          # Mark dead after this time
heartbeat_interval = 5.0     # Heartbeat frequency
```

## ğŸ¯ Design Decisions

### Why UDP?

- **Lower Latency**: No TCP handshake overhead
- **Multicast Ready**: Can extend to multicast discovery
- **Explicit Control**: Custom reliability semantics
- **Mesh-Friendly**: No connection state overhead

### Why CRDT (LWW-Register)?

- **Automatic Conflict Resolution**: No coordination needed
- **Commutative & Associative**: Order-independent merging
- **Simple Implementation**: Easy to understand and debug
- **Proven**: Used in production systems (Riak, Cassandra)

### Why Not...?

- **TCP**: Requires connection state, not mesh-friendly
- **Raft/Paxos**: Requires leader, not multi-master
- **Two-Phase Commit**: Too much coordination for unreliable networks
- **Gossip Alone**: Needs reliability layer

## ğŸ› Known Limitations

1. **State Size**: Large states (>1MB) may fragment across packets
2. **Clock Skew**: Relies on monotonic timestamps (use NTP in production)
3. **Memory**: Keeps full history in vector clocks (can grow unbounded)
4. **Security**: No encryption or authentication (add TLS for production)

## ğŸš€ Future Enhancements

- [ ] State compression (gzip for large payloads)
- [ ] Delta synchronization (send only diffs)
- [ ] Merkle trees for efficient sync
- [ ] Multicast discovery for faster peer finding
- [ ] TLS encryption for secure communication
- [ ] Metrics and monitoring endpoints
- [ ] Persistent storage (currently in-memory only)
- [ ] Causal consistency with version vectors

## ğŸ“š References

### CRDT Theory
- Shapiro et al., "A comprehensive study of CRDTs" (2011)
- [CRDT.tech](https://crdt.tech/)

### Distributed Systems
- Kleppmann, "Designing Data-Intensive Applications"
- Vogels, "Eventually Consistent" (2008)

### Network Protocols
- RFC 768 (UDP)
- [Gossip Protocols](https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf)

## ğŸ“ License

MIT License - Feel free to use in your projects

## ğŸ‘¥ Contributing

This is a demonstration project for educational purposes. Feel free to fork and extend!

## ğŸ“ Educational Value

This project demonstrates:
- âœ… Socket programming (UDP)
- âœ… Application-layer protocols
- âœ… Distributed state management
- âœ… Conflict-free replication
- âœ… Failure detection
- âœ… Network simulation
- âœ… Concurrent programming
- âœ… System design principles

Perfect for learning distributed systems concepts hands-on!
